---
title: "Model Selection"
description: "Frequently asked questions about choosing and comparing AI models"
---

<AccordionGroup>
  <Accordion title="How do I choose the right AI model?">
    The best model depends on your use case. You can browse the full list on the [Supported AI Models](/supported-models) page, or call the `GET /v1/models` endpoint to get the current list programmatically.

    As a starting point, `gemini-2.5-flash` offers a good balance of speed and quality for most tasks.
  </Accordion>

  <Accordion title="How do different models compare in response speed?">
    Response speed varies based on model size, task complexity, and server load. Smaller models (e.g., `google/gemma-3-12b-it`) tend to respond faster, while larger models (e.g., `Qwen/Qwen3-235B-A22B-Instruct-2507`) provide more capable responses. We recommend benchmarking with your specific use cases to find the best fit.
  </Accordion>

  <Accordion title="How can I check which models are available?">
    Call the `GET /v1/models` endpoint to get a real-time list of all available models:

    ```bash
    curl https://gateway.iotex.ai/v1/models \
      -H "Authorization: Bearer your-api-key"
    ```
  </Accordion>

  <Accordion title="Can I switch models without changing my code?">
    Yes. Since all models use the same OpenAI-compatible API format, switching models is as simple as changing the `model` parameter in your request. No other code changes are needed.
  </Accordion>
</AccordionGroup>
